# GAN_model_for_handwriting_generation
GAN consists of two components: generator and discriminator. However, unlike in auto-encoder, this is an adversarial model where the two components learn by fighting against each other.
Generator synthesizes samples from a simple random probability distribution and the task of discriminator is to discriminate between the probability distribution of the real and fake image. Therefore, the discriminator is supervised for classification, but the generator learning is unsupervised.

Training procedure on different components: Discriminator training is like a simple binary classifier, that involves freezing the generator and then feed it with labelled real and fake images in order to train it to classify if the data sample is from real data distribution or generated data distribution. Then compute the gradient of the loss function w.r.t. the discriminator parameters to get parameters that maximize the objective function. This training is performed for certain epochs. Basically, it is ascending the gradient descent to find maximum value.Generator training involves freezing the discriminator after it has been trained, generate samples and pass through the discriminator to get the feedback. The result from discriminator is used to update the weights and biases using backpropagation to fine tune the generator and minimize loss, to have the value of fake image as close to 1 as possible; to fool the discriminator. Basically, it is descending the gradient descent to find minimum value.

The python codes here show how to train and test GAN model to generate handwriting along with supplementary module to plot the digits and check the weights.
